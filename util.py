import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset
import torchvision.transforms as transforms

import os
from PIL import Image
import pandas as pd
import math
import warnings
import random
from PIL import ImageFilter, ImageOps
import numpy as np

class GaussianBlur(object):
    """Apply Gaussian Blur to the PIL image"""
    def __init__(self, p = 0.5, radius_min = 0.1, radius_max = 2.):
        self.prob = p
        self.radius_min = radius_min
        self.radius_max = radius_max
        
    def __call__(self, img):
        do_it = random.random() <= self.prob
        if not do_it:
            return img
        return img.filter(ImageFilter.GaussianBlur(radius = random.uniform(self.radius_min, self.radius_max)))
    
class Solarization(object):
    """Apply Solarization to the PIL image"""
    def __init__(self, p):
        self.p = p
        
    def __call__(self, img):
        if random.random() <self.p:
            return ImageOps.solarize(img)
        else:
            return img

class DataAugmentationDINO(object):
    def __init__(self, size, global_crops_scale, local_crops_scale, local_crops_number):
        flip_and_color_jitter = transforms.Compose([transforms.RandomHorizontalFlip(p = 0.5),
                                                    transforms.RandomApply([transforms.ColorJitter(brightness = 0.4, contrast = 0.4, saturation = 0.2, hue = 0.1)],
                                                                           p = 0.8),
                                                    transforms.RandomGrayscale(p = 0.2)
                                                    ])
        normalize = transforms.Compose([transforms.ToTensor(),
                                        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])
        
        # first global crop
        self.global_1 = transforms.Compose([
            transforms.RandomResizedCrop(size, scale = global_crops_scale, interpolation = Image.BICUBIC),
                                                     flip_and_color_jitter,
                                                     GaussianBlur(0.1),
                                                     normalize])
        
        # second global crop
        self.global_2 = transforms.Compose([
            transforms.RandomResizedCrop(size, scale = global_crops_scale, interpolation = Image.BICUBIC),
                                                     flip_and_color_jitter,
                                                     Solarization(0.2),
                                                     GaussianBlur(0.1),
                                                     normalize])
        
        # transformation for the local small crops
        self.n_local_crops = local_crops_number
        self.local = transforms.Compose([transforms.RandomResizedCrop(size, scale = local_crops_scale, interpolation = Image.BICUBIC),
                                                   flip_and_color_jitter,
                                                   GaussianBlur(0.1),
                                                   normalize])
        
    def __call__(self, image):
        crops = []
        crops.append(self.global_1(image))
        crops.append(self.global_2(image))
        crops.extend([self.local(image) for _ in range(self.n_local_crops)])
        return crops
        
class DPImageDataset(Dataset):
    def __init__(self, csv_file, transform = None):
        self.path = pd.read_csv(csv_file)
        self.transform = transform
        
    def __len__(self):
        return len(self.path)
    
    def __getitem__(self, idx):
        # try:
        image = self.path['imgpath'][idx]
        image = Image.open(image).convert('RGB')
        y_label = int(self.path['encoded_labels'][idx])
        if self.transform:
            image = self.transform(image)

        return image, y_label
        
        # except:
            # image = ''.join(['.', self.path['imgpath'][idx]])
            # image = Image.open(image).convert('RGB')
            # y_label = int(self.path['encoded_labels'][idx])
            # if self.transform:
            #     image = self.transform(image)

            # return image, y_label
            # pass
        
def trunc_normal_(tensor, mean = 0., std = 1., a = -2., b = 2.):
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

def _no_grad_trunc_normal_(tensor, mean, std, a,b):
    def norm_cdf(x):
        return (1. + math.erf(x / math.sqrt(2.))) / 2.
    
    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)
        
    with torch.no_grad():
        """Values are generated by using a truncated uniform distribution and then using
        the inverse CDF for the normal distribution.
        Get upper and lower cdf values."""
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)
        
        #Uniformly fill tensor with values form [l, u], then translate to [2l - 1, 2u - 1]
        tensor.uniform_(2*l - 1, 2*u - 1)
        
        #Use inverse cdf transform for normal distribution to get truncated standard normal
        tensor.erfinv_()
        
        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)
        
        #Clamp to ensure it;s in the proper range
        tensor.clamp_(min = a, max = b)
        return tensor
    
def cosine_scheduler(base_value, final_value, epochs, niter_per_ep, warmup_epochs=0, start_warmup_value=0):
    warmup_schedule = np.array([])
    warmup_iters = warmup_epochs * niter_per_ep
    if warmup_epochs > 0:
        warmup_schedule = np.linspace(start_warmup_value, base_value, warmup_iters)

    iters = np.arange(epochs * niter_per_ep - warmup_iters)
    schedule = final_value + 0.5 * (base_value - final_value) * (1 + np.cos(np.pi * iters / len(iters)))

    schedule = np.concatenate((warmup_schedule, schedule))
    assert len(schedule) == epochs * niter_per_ep
    return schedule

def restart_from_checkpoint(ckp_path, run_variables=None, **kwargs):
    """
    Re-start from checkpoint
    """
    if not os.path.isfile(ckp_path):
        return
    print("Found checkpoint at {}".format(ckp_path))

    # open checkpoint file
    checkpoint = torch.load(ckp_path, map_location="cpu")

    # key is what to look for in the checkpoint file
    # value is the object to load
    # example: {'state_dict': model}
    for key, value in kwargs.items():
        if key in checkpoint and value is not None:
            try:
                msg = value.load_state_dict(checkpoint[key], strict=False)
                print("=> loaded '{}' from checkpoint '{}' with msg {}".format(key, ckp_path, msg))
            except TypeError:
                try:
                    msg = value.load_state_dict(checkpoint[key])
                    print("=> loaded '{}' from checkpoint: '{}'".format(key, ckp_path))
                except ValueError:
                    print("=> failed to load '{}' from checkpoint: '{}'".format(key, ckp_path))
        else:
            print("=> key '{}' not found in checkpoint: '{}'".format(key, ckp_path))

    # re load variable important for the run
    if run_variables is not None:
        for var_name in run_variables:
            if var_name in checkpoint:
                run_variables[var_name] = checkpoint[var_name]

def get_world_size():
    return torch.cuda.device_count()

def clip_gradients(model, clip):
    norms = []
    for name, p in model.named_parameters():
        if p.grad is not None:
            param_norm = p.grad.data.norm(2)
            norms.append(param_norm.item())
            clip_coef = clip / (param_norm + 1e-6)
            if clip_coef < 1:
                p.grad.data.mul_(clip_coef)
    return norms

def cancel_gradients_last_layer(epoch, model, freeze_last_layer):
    if epoch >= freeze_last_layer:
        return
    for n,p in model.named_parameters():
        if "last_layer" in n:
            p.grad = None